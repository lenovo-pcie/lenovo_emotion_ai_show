import cv2
import sqlite3
import os
from datetime import datetime
from PIL import Image
import numpy as np
import base64
import io
import time
import signal
import sys
import threading
import multiprocessing
import json
import getpass

# è®¾ç½®OpenCVæ—¥å¿—çº§åˆ«ï¼ŒæŠ‘åˆ¶è°ƒè¯•ä¿¡æ¯
try:
    # å°è¯•ä¸åŒçš„æ—¥å¿—çº§åˆ«å¸¸é‡åç§°
    if hasattr(cv2, 'LOG_LEVEL_SILENT'):
        cv2.setLogLevel(cv2.LOG_LEVEL_SILENT)
    elif hasattr(cv2, 'utils_logging_setLogLevel'):
        cv2.utils_logging_setLogLevel(cv2.utils_logging.LOG_LEVEL_SILENT)
    elif hasattr(cv2, 'setLogLevel'):
        # ç›´æ¥ä½¿ç”¨æ•°å­—å€¼
        cv2.setLogLevel(0)  # 0 = SILENT
    else:
        print("âš ï¸ å½“å‰OpenCVç‰ˆæœ¬ä¸æ”¯æŒæ—¥å¿—çº§åˆ«è®¾ç½®")
except Exception as e:
    print(f"âš ï¸ è®¾ç½®OpenCVæ—¥å¿—çº§åˆ«å¤±è´¥: {e}")

# ç®€åŒ–ï¼šä¸å†ä¾èµ–å¤–éƒ¨æ‘„åƒå¤´é…ç½®æ¨¡å—
def print_camera_config():
    pass

def get_system_camera_count():
    """ç®€åŒ–ï¼šå‡å®šå­˜åœ¨é»˜è®¤å†…ç½®æ‘„åƒå¤´ç´¢å¼•0"""
    return 1, [0]

class EmotionAnalyzerBase:
    def __init__(self, api_key, camera_backend=cv2.CAP_ANY, camera_index=0, model_name="Unknown"):
        """åˆå§‹åŒ–æƒ…ç»ªåˆ†æå™¨åŸºç¡€ç±»"""
        self.api_key = api_key
        self.camera_backend = camera_backend
        self.camera_index = camera_index
        self.model_name = model_name
        # Store database in the current working directory
        self.db_path = os.path.join(os.getcwd(), 'emotion_data.db')
        self.running = True
        self.init_database()
        
        # è®¾ç½®ä¿¡å·å¤„ç†ï¼Œä¼˜é›…é€€å‡º
        signal.signal(signal.SIGINT, self.signal_handler)
        signal.signal(signal.SIGTERM, self.signal_handler)
        
        print(f"âœ… {model_name}æƒ…ç»ªåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“· æ‘„åƒå¤´ç´¢å¼•: {self.camera_index}")
        print(f"ğŸ”§ æ‘„åƒå¤´åç«¯: {self.camera_backend}")
        print(f"ğŸ—„ï¸ æ•°æ®åº“: {self.db_path}")
    
    def signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å‡½æ•°ï¼Œç”¨äºä¼˜é›…é€€å‡º"""
        print(f"\nğŸ›‘ æ”¶åˆ°é€€å‡ºä¿¡å· {signum}ï¼Œæ­£åœ¨åœæ­¢ç¨‹åº...")
        self.running = False
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='emotion_records'")
        table_exists = cursor.fetchone() is not None
        
        if table_exists:
            # æ£€æŸ¥æ˜¯å¦æœ‰emotion_levelå­—æ®µ
            cursor.execute("PRAGMA table_info(emotion_records)")
            columns = [col[1] for col in cursor.fetchall()]
            
            if 'emotion_level' not in columns:
                print("ğŸ”„ æ£€æµ‹åˆ°æ—§ç‰ˆæœ¬æ•°æ®åº“ï¼Œæ­£åœ¨æ·»åŠ emotion_levelå­—æ®µ...")
                cursor.execute('ALTER TABLE emotion_records ADD COLUMN emotion_level REAL DEFAULT 0.0')
                print("âœ… emotion_levelå­—æ®µæ·»åŠ å®Œæˆ")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å±å¹•å†…å®¹ç›¸å…³å­—æ®µ
            if 'app_name' not in columns:
                print("ğŸ”„ æ£€æµ‹åˆ°æ—§ç‰ˆæœ¬æ•°æ®åº“ï¼Œæ­£åœ¨æ·»åŠ å±å¹•å†…å®¹å­—æ®µ...")
                cursor.execute('ALTER TABLE emotion_records ADD COLUMN app_name TEXT DEFAULT "æœªçŸ¥åº”ç”¨"')
                cursor.execute('ALTER TABLE emotion_records ADD COLUMN app_category TEXT DEFAULT "å…¶ä»–"')
                cursor.execute('ALTER TABLE emotion_records ADD COLUMN content_description TEXT DEFAULT "æ— æ³•è¯†åˆ«å½“å‰åº”ç”¨å’Œå†…å®¹"')
                cursor.execute('ALTER TABLE emotion_records ADD COLUMN screen_path TEXT')
                print("âœ… å±å¹•å†…å®¹å­—æ®µæ·»åŠ å®Œæˆ")
        else:
            # æ–°å»ºè¡¨æ—¶ç›´æ¥å¸¦æ‰€æœ‰å­—æ®µ
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS emotion_records (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    emotion TEXT NOT NULL,
                    confidence REAL NOT NULL,
                    has_face BOOLEAN NOT NULL,
                    image_path TEXT,
                    username TEXT,
                    emotion_level REAL DEFAULT 0.0,
                    app_name TEXT DEFAULT "æœªçŸ¥åº”ç”¨",
                    app_category TEXT DEFAULT "å…¶ä»–",
                    content_description TEXT DEFAULT "æ— æ³•è¯†åˆ«å½“å‰åº”ç”¨å’Œå†…å®¹",
                    screen_path TEXT
                )
            ''')
        conn.commit()
        conn.close()
    
    def save_image(self, image, timestamp, image_index=None):
        """ä¿å­˜å›¾åƒåˆ°æœ¬åœ°ï¼Œç›®å½•ç»“æ„ä¸º å¹´/æœˆ/æ—¥/å°æ—¶/"""
        dt = datetime.strptime(timestamp, "%Y%m%d-%H%M%S")
        # è·å–å½“å‰ç›®å½•çš„çˆ¶ç›®å½•
        parent_dir = os.path.dirname(os.path.abspath('.'))
        dir_path = os.path.join(
            parent_dir,
            'testimages',
            dt.strftime('%Y'),
            dt.strftime('%m'),
            dt.strftime('%d'),
            dt.strftime('%H')
        )
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)
        
        # å¦‚æœæœ‰å›¾ç‰‡åºå·ï¼Œæ·»åŠ åˆ°æ–‡ä»¶åä¸­
        if image_index is not None:
            filename = f'{timestamp}_{image_index:02d}.jpg'
        else:
            filename = f'{timestamp}.jpg'
            
        image_path = os.path.join(dir_path, filename)
        cv2.imwrite(image_path, image)
        return image_path
    
    def image_to_base64(self, image):
        """å°†OpenCVå›¾åƒè½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²"""
        # è½¬æ¢BGRåˆ°RGB
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(image_rgb)
        
        # è½¬æ¢ä¸ºbase64
        buffer = io.BytesIO()
        pil_image.save(buffer, format='JPEG')
        img_str = base64.b64encode(buffer.getvalue()).decode()
        return img_str
    
    def capture_screen(self, timestamp):
        """æ•è·å±å¹•æˆªå›¾"""
        try:
            import pyautogui
            print(f"ğŸ–¥ï¸ æ­£åœ¨æ•è·å±å¹•æˆªå›¾...")
            
            # æ•è·å±å¹•æˆªå›¾
            screenshot = pyautogui.screenshot()
            
            # è½¬æ¢ä¸ºOpenCVæ ¼å¼
            screenshot_cv = cv2.cvtColor(np.array(screenshot), cv2.COLOR_RGB2BGR)
            
            # ä¿å­˜æˆªå›¾
            saved_screen_path = self.save_screen(screenshot_cv, timestamp)
            
            print(f"âœ… å±å¹•æˆªå›¾å·²ä¿å­˜åˆ°: {saved_screen_path}")
            return screenshot_cv, saved_screen_path
            
        except ImportError:
            print("âŒ æœªå®‰è£…pyautoguiï¼Œæ— æ³•æ•è·å±å¹•æˆªå›¾")
            return None, None
        except Exception as e:
            print(f"âŒ å±å¹•æˆªå›¾æ•è·å¤±è´¥: {e}")
            return None, None
    
    def save_screen(self, screen_image, timestamp):
        """ä¿å­˜å±å¹•æˆªå›¾åˆ°æœ¬åœ°ï¼Œä¸è§†é¢‘ä¿å­˜åˆ°ç›¸åŒè·¯å¾„"""
        dt = datetime.strptime(timestamp, "%Y%m%d-%H%M%S")
        # è·å–å½“å‰ç›®å½•çš„çˆ¶ç›®å½•
        parent_dir = os.path.dirname(os.path.abspath('.'))
        dir_path = os.path.join(
            parent_dir,
            'testvideos',  # ä½¿ç”¨ä¸è§†é¢‘ç›¸åŒçš„ç›®å½•å­˜å‚¨å±å¹•æˆªå›¾
            dt.strftime('%Y'),
            dt.strftime('%m'),
            dt.strftime('%d'),
            dt.strftime('%H')
        )
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)
        
        filename = f'{timestamp}_screen.png'
        screen_path = os.path.join(dir_path, filename)
        cv2.imwrite(screen_path, screen_image)
        return screen_path
    
    def analyze_emotion(self, images):
        """åˆ†ææƒ…ç»ª - å­ç±»å¿…é¡»å®ç°æ­¤æ–¹æ³•ï¼Œæ¥æ”¶å›¾ç‰‡åˆ—è¡¨"""
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç°analyze_emotionæ–¹æ³•")
    
    def _get_default_result(self):
        """è·å–é»˜è®¤ç»“æœ"""
        return {
            "has_face": False,
            "emotion": "æ— ",
            "confidence": 0.0,
            "emotion_level": 0.0
        }
    
    def _get_default_screen_result(self):
        """è·å–é»˜è®¤çš„å±å¹•åˆ†æç»“æœ"""
        return {
            "app_name": "æœªçŸ¥åº”ç”¨",
            "app_category": "å…¶ä»–",
            "content_description": "æ— æ³•è¯†åˆ«å½“å‰åº”ç”¨å’Œå†…å®¹"
        }
    
    def save_to_database(self, timestamp, emotion, confidence, has_face, image_path=None, emotion_level=0.0, 
                        app_name="æœªçŸ¥åº”ç”¨", app_category="å…¶ä»–", content_description="æ— æ³•è¯†åˆ«å½“å‰åº”ç”¨å’Œå†…å®¹", screen_path=None):
        """ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“ï¼Œå¢åŠ å±å¹•å†…å®¹å­—æ®µ"""
        username = None
        try:
            username = getpass.getuser()
        except Exception:
            username = os.environ.get('USERNAME') or os.environ.get('USER') or 'unknown'
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO emotion_records (timestamp, emotion, confidence, has_face, image_path, username, emotion_level, 
                                       app_name, app_category, content_description, screen_path)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (timestamp, emotion, confidence, has_face, image_path, username, emotion_level, 
              app_name, app_category, content_description, screen_path))
        conn.commit()
        conn.close()
    
    def capture_video(self, duration=3.0):
        """ä»æ‘„åƒå¤´å½•åˆ¶æŒ‡å®šæ—¶é•¿çš„è§†é¢‘"""
        import cv2
        import tempfile
        import os
        
        print(f"ğŸ¬ å¼€å§‹å½•åˆ¶ {duration} ç§’è§†é¢‘...")
        
        # æ‰“å¼€æ‘„åƒå¤´
        cap = cv2.VideoCapture(self.camera_index, self.camera_backend)
        if not cap.isOpened():
            raise Exception(f"æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {self.camera_index}")
        
        # è·å–æ‘„åƒå¤´å‚æ•°
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        if fps <= 0:
            fps = 30  # é»˜è®¤å¸§ç‡
        
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        # åˆ›å»ºä¸´æ—¶è§†é¢‘æ–‡ä»¶
        temp_video_path = tempfile.mktemp(suffix='.mp4')
        
        # è®¾ç½®è§†é¢‘ç¼–ç å™¨
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(temp_video_path, fourcc, fps, (width, height))
        
        if not out.isOpened():
            cap.release()
            raise Exception("æ— æ³•åˆ›å»ºè§†é¢‘æ–‡ä»¶")
        
        # è®¡ç®—éœ€è¦å½•åˆ¶çš„å¸§æ•°
        total_frames = int(fps * duration)
        frame_count = 0
        
        try:
            while frame_count < total_frames:
                ret, frame = cap.read()
                if not ret:
                    print(f"âš ï¸ ç¬¬ {frame_count + 1} å¸§è¯»å–å¤±è´¥")
                    break
                
                # å†™å…¥è§†é¢‘å¸§
                out.write(frame)
                frame_count += 1
                
                # æ˜¾ç¤ºå½•åˆ¶è¿›åº¦
                if frame_count % fps == 0:
                    elapsed_time = frame_count / fps
                    print(f"   ğŸ“¹ å·²å½•åˆ¶ {elapsed_time:.1f}/{duration} ç§’")
        
        finally:
            # é‡Šæ”¾èµ„æº
            cap.release()
            out.release()
        
        print(f"âœ… è§†é¢‘å½•åˆ¶å®Œæˆ: {frame_count} å¸§, {frame_count/fps:.2f} ç§’")
        return temp_video_path
    
    def video_to_base64(self, video_path):
        """å°†è§†é¢‘æ–‡ä»¶è½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²"""
        import base64
        
        try:
            with open(video_path, 'rb') as video_file:
                video_data = video_file.read()
                video_base64 = base64.b64encode(video_data).decode('utf-8')
                return video_base64
        except Exception as e:
            raise Exception(f"è§†é¢‘æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
    
    def extract_frames_from_video(self, video_path, num_frames=6):
        """ä»è§†é¢‘ä¸­æå–æŒ‡å®šæ•°é‡çš„å¸§"""
        import cv2
        
        print(f"ğŸ¬ ä»è§†é¢‘ä¸­æå– {num_frames} å¸§å›¾åƒ...")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise Exception("æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
        
        # è·å–è§†é¢‘ä¿¡æ¯
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        duration = total_frames / fps
        
        print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {total_frames} å¸§, {fps:.1f} fps, {duration:.2f} ç§’")
        
        # è®¡ç®—æå–å¸§çš„é—´éš”
        if total_frames <= num_frames:
            # å¦‚æœæ€»å¸§æ•°å°‘äºéœ€è¦çš„å¸§æ•°ï¼Œæå–æ‰€æœ‰å¸§
            frame_indices = list(range(total_frames))
        else:
            # å¹³å‡é—´éš”æå–å¸§
            step = total_frames / num_frames
            frame_indices = [int(i * step) for i in range(num_frames)]
        
        frames = []
        for i, frame_idx in enumerate(frame_indices):
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            if ret:
                frames.append(frame)
                print(f"   âœ… æå–ç¬¬ {i+1}/{num_frames} å¸§ (åŸå§‹å¸§ {frame_idx})")
            else:
                print(f"   âŒ æå–ç¬¬ {i+1}/{num_frames} å¸§å¤±è´¥")
        
        cap.release()
        
        if not frames:
            raise Exception("æœªèƒ½ä»è§†é¢‘ä¸­æå–åˆ°ä»»ä½•å¸§")
        
        print(f"âœ… æˆåŠŸæå– {len(frames)} å¸§å›¾åƒ")
        return frames
    
    def analyze_video_emotion(self, video_path):
        """åˆ†æè§†é¢‘ä¸­çš„æƒ…ç»ª - ä»è§†é¢‘ä¸­æå–6å¸§å›¾åƒï¼Œä¼ ç»™æ¨¡å‹åˆ†æ"""
        try:
            print(f"ğŸ¬ æ­£åœ¨ä»è§†é¢‘ä¸­æå–6å¸§å›¾åƒ...")
            
            # ä»è§†é¢‘ä¸­æå–6å¸§
            frames = self.extract_frames_from_video(video_path, num_frames=6)
            
            print(f"ğŸ“Š æˆåŠŸæå– {len(frames)} å¸§å›¾åƒ")
            
            # å°†å¸§å›¾åƒä¼ ç»™æ¨¡å‹è¿›è¡Œåˆ†æ
            analysis_results = self.analyze_emotion(frames)
            
            print(f"ğŸ“Š æ¨¡å‹è¿”å›äº† {len(analysis_results)} ä¸ªåˆ†æç»“æœ")
            for i, result in enumerate(analysis_results):
                print(f"   âœ… ç¬¬ {i+1} ä¸ªç»“æœ: {result['emotion']} (çº§åˆ«: {result['emotion_level']:.2f})")
            
            return analysis_results
            
        except Exception as e:
            print(f"âŒ è§†é¢‘åˆ†æå¤±è´¥: {e}")
            return [self._get_default_result() for _ in range(6)]
    
    def _calculate_average_result(self, results):
        """è®¡ç®—å¤šä¸ªåˆ†æç»“æœçš„å¹³å‡å€¼"""
        if not results:
            return self._get_default_result()
        
        # ç»Ÿè®¡æƒ…ç»ªç±»å‹
        emotion_counts = {}
        total_confidence = 0
        total_emotion_level = 0
        total_has_face = 0
        
        for result in results:
            emotion = result.get('emotion', 'æ— ')
            confidence = result.get('confidence', 0.0)
            emotion_level = result.get('emotion_level', 0.0)
            has_face = result.get('has_face', False)
            
            emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
            total_confidence += confidence
            total_emotion_level += emotion_level
            if has_face:
                total_has_face += 1
        
        # é€‰æ‹©å‡ºç°æ¬¡æ•°æœ€å¤šçš„æƒ…ç»ª
        if emotion_counts:
            most_common_emotion = max(emotion_counts.items(), key=lambda x: x[1])
            dominant_emotion = most_common_emotion[0]
        else:
            dominant_emotion = 'æ— '
        
        # è®¡ç®—å¹³å‡å€¼
        avg_confidence = total_confidence / len(results)
        avg_emotion_level = total_emotion_level / len(results)
        avg_has_face = total_has_face / len(results) > 0.5  # è¶…è¿‡ä¸€åŠçš„å¸§æœ‰äººè„¸
        
        return {
            "has_face": avg_has_face,
            "emotion": dominant_emotion,
            "confidence": avg_confidence,
            "emotion_level": avg_emotion_level
        }
    
    def process_video_capture(self):
        """å®Œæ•´çš„è§†é¢‘æ•è·å’Œåˆ†ææµç¨‹"""
        try:
            # è®°å½•æ€»ä½“å¼€å§‹æ—¶é—´
            total_start_time = time.time()
            
            # ç”Ÿæˆæ—¶é—´æˆ³
            timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
            
            # å½•åˆ¶è§†é¢‘
            print(f"[{timestamp}] æ­£åœ¨å½•åˆ¶è§†é¢‘...")
            capture_start = time.time()
            video_path = self.capture_video(duration=3.0)
            capture_time = time.time() - capture_start
            print(f"[{timestamp}] è§†é¢‘å½•åˆ¶å®Œæˆ - è€—æ—¶: {capture_time:.2f}ç§’")
            
            # æ•è·å±å¹•æˆªå›¾
            print(f"[{timestamp}] æ­£åœ¨æ•è·å±å¹•æˆªå›¾...")
            screen_start = time.time()
            screen_image, screen_path = self.capture_screen(timestamp)
            screen_time = time.time() - screen_start
            print(f"[{timestamp}] å±å¹•æˆªå›¾å®Œæˆ - è€—æ—¶: {screen_time:.2f}ç§’")
            
            # åˆ†æè§†é¢‘æƒ…ç»ª
            print(f"[{timestamp}] æ­£åœ¨åˆ†æè§†é¢‘æƒ…ç»ª...")
            analysis_start = time.time()
            analysis_results = self.analyze_video_emotion(video_path)
            analysis_time = time.time() - analysis_start
            print(f"[{timestamp}] è§†é¢‘æƒ…ç»ªåˆ†æå®Œæˆ - è€—æ—¶: {analysis_time:.2f}ç§’")
            
            # åˆ†æå±å¹•å†…å®¹ï¼ˆå¦‚æœæ”¯æŒï¼‰
            screen_analysis_result = None
            if screen_image is not None and hasattr(self, 'analyze_screen_content'):
                print(f"[{timestamp}] æ­£åœ¨åˆ†æå±å¹•å†…å®¹...")
                screen_analysis_start = time.time()
                try:
                    screen_analysis_result = self.analyze_screen_content(screen_image)
                    screen_analysis_time = time.time() - screen_analysis_start
                    print(f"[{timestamp}] å±å¹•å†…å®¹åˆ†æå®Œæˆ - è€—æ—¶: {screen_analysis_time:.2f}ç§’")
                except Exception as e:
                    print(f"[{timestamp}] å±å¹•å†…å®¹åˆ†æå¤±è´¥: {e}")
                    screen_analysis_result = self._get_default_screen_result()
            else:
                screen_analysis_result = self._get_default_screen_result()
            
            # ä¿å­˜è§†é¢‘æ–‡ä»¶
            print(f"[{timestamp}] æ­£åœ¨ä¿å­˜è§†é¢‘...")
            save_start = time.time()
            saved_video_path = self.save_video(video_path, timestamp)
            save_time = time.time() - save_start
            print(f"[{timestamp}] è§†é¢‘å·²ä¿å­˜åˆ°: {saved_video_path} - è€—æ—¶: {save_time:.2f}ç§’")
            
            # ä¿å­˜åˆ°æ•°æ®åº“ - å¤„ç†6ä¸ªåˆ†æç»“æœ
            db_start = time.time()
            saved_results = []
            
            for i, analysis_result in enumerate(analysis_results):
                # ä¸ºæ¯ä¸ªç»“æœç”Ÿæˆå”¯ä¸€çš„æ—¶é—´æˆ³
                result_timestamp = f"{timestamp}_{i:02d}"
                
                # ä¿å­˜åˆ°æ•°æ®åº“ï¼ŒåŒ…å«å±å¹•å†…å®¹ä¿¡æ¯
                self.save_to_database(
                    timestamp=result_timestamp,
                    emotion=analysis_result['emotion'],
                    confidence=analysis_result['confidence'],
                    has_face=analysis_result['has_face'],
                    image_path=saved_video_path,  # ä½¿ç”¨è§†é¢‘è·¯å¾„æ›¿ä»£å›¾ç‰‡è·¯å¾„
                    emotion_level=analysis_result['emotion_level'],
                    app_name=screen_analysis_result.get('app_name', 'æœªçŸ¥åº”ç”¨'),
                    app_category=screen_analysis_result.get('app_category', 'å…¶ä»–'),
                    content_description=screen_analysis_result.get('content_description', 'æ— æ³•è¯†åˆ«å½“å‰åº”ç”¨å’Œå†…å®¹'),
                    screen_path=screen_path
                )
                
                saved_results.append({
                    'timestamp': result_timestamp,
                    'result': analysis_result,
                    'screen_analysis': screen_analysis_result
                })
                
                # æ‰“å°æ¯ä¸ªç»“æœçš„è¯¦ç»†ä¿¡æ¯
                emotion_info = f"æƒ…ç»ª={analysis_result['emotion']}"
                if analysis_result['has_face'] and analysis_result['emotion'] != 'æ— ':
                    emotion_info += f", çº§åˆ«={analysis_result['emotion_level']:.2f}"
                print(f"[{timestamp}] ç»“æœ {i+1}/6: äººè„¸={analysis_result['has_face']}, {emotion_info}, ç½®ä¿¡åº¦={analysis_result['confidence']:.2f}")
            
            # æ‰“å°å±å¹•åˆ†æç»“æœ
            if screen_analysis_result:
                print(f"[{timestamp}] å±å¹•åˆ†æ: åº”ç”¨={screen_analysis_result.get('app_name', 'æœªçŸ¥')}, "
                      f"åˆ†ç±»={screen_analysis_result.get('app_category', 'å…¶ä»–')}, "
                      f"æè¿°={screen_analysis_result.get('content_description', 'æ— æè¿°')}")
            
            db_time = time.time() - db_start
            print(f"[{timestamp}] æ•°æ®ä¿å­˜å®Œæˆ - è€—æ—¶: {db_time:.2f}ç§’")
            
            # è®¡ç®—æ€»è€—æ—¶
            total_time = time.time() - total_start_time
            
            print(f"[{timestamp}] è§†é¢‘å’Œå±å¹•åˆ†æå®Œæˆï¼")
            print(f"[{timestamp}] æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")
            print(f"[{timestamp}] æ€§èƒ½ä¼˜åŒ–: è§†é¢‘åˆ†æåœ¨ä¿å­˜å‰å®Œæˆï¼ŒèŠ‚çœäº†å…ˆä¿å­˜å†è¯»å–çš„æ—¶é—´")
            print(f"[{timestamp}] ä»è§†é¢‘ä¸­æå–6å¸§å¹¶è¿”å›6ä¸ªåˆ†æç»“æœ")
            print(f"[{timestamp}] åŒæ—¶åˆ†æå±å¹•å†…å®¹å¹¶ä¿å­˜åˆ°åŒä¸€æ¡è®°å½•")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.remove(video_path)
            except:
                pass
            
            return {
                'timestamp': timestamp,
                'analysis_results': saved_results,
                'video_path': saved_video_path,
                'screen_path': screen_path,
                'screen_analysis': screen_analysis_result,
                'timing': {
                    'capture': capture_time,
                    'screen': screen_time,
                    'analysis': analysis_time,
                    'save': save_time,
                    'database': db_time,
                    'total': total_time
                }
            }
            
        except Exception as e:
            print(f"[{datetime.now().strftime('%Y%m%d-%H%M%S')}] è§†é¢‘å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            return None
    
    def save_video(self, video_path, timestamp):
        """ä¿å­˜è§†é¢‘åˆ°æœ¬åœ°ï¼Œç›®å½•ç»“æ„ä¸º å¹´/æœˆ/æ—¥/å°æ—¶/"""
        dt = datetime.strptime(timestamp, "%Y%m%d-%H%M%S")
        # è·å–å½“å‰ç›®å½•çš„çˆ¶ç›®å½•
        parent_dir = os.path.dirname(os.path.abspath('.'))
        dir_path = os.path.join(
            parent_dir,
            'testvideos',  # ä½¿ç”¨testvideosç›®å½•å­˜å‚¨è§†é¢‘
            dt.strftime('%Y'),
            dt.strftime('%m'),
            dt.strftime('%d'),
            dt.strftime('%H')
        )
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)
        
        filename = f'{timestamp}_video.mp4'
        saved_video_path = os.path.join(dir_path, filename)
        
        # å¤åˆ¶è§†é¢‘æ–‡ä»¶
        import shutil
        shutil.copy2(video_path, saved_video_path)
        return saved_video_path

    def run_continuous_video(self, interval_minutes=1.0):
        """è¿ç»­è¿è¡Œæ¨¡å¼ - è§†é¢‘è¯†åˆ«ç‰ˆæœ¬"""
        print("ğŸš€ æƒ…ç»ªåˆ†æç³»ç»Ÿå¯åŠ¨ - è§†é¢‘è¯†åˆ«æ¨¡å¼")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {self.model_name}")
        print(f"ğŸ“· æ‘„åƒå¤´ç´¢å¼•: {self.camera_index}")
        print(f"â° æ•è·é—´éš”: {interval_minutes} åˆ†é’Ÿ")
        print(f"ğŸ¬ æ¯æ¬¡å½•åˆ¶: 3ç§’è§†é¢‘")
        print("ğŸ›‘ æŒ‰ Ctrl+C åœæ­¢ç¨‹åº")
        print("="*60)
        
        interval_seconds = int(interval_minutes * 60)  # è½¬æ¢ä¸ºæ•´æ•°
        cycle_count = 0
        
        while self.running:
            try:
                cycle_count += 1
                current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                print(f"\nğŸ”„ ç¬¬ {cycle_count} æ¬¡å¾ªç¯ - {current_time}")
                print("-" * 40)
                
                # æ‰§è¡Œè§†é¢‘æ•è·å’Œåˆ†æ
                result = self.process_video_capture()
                
                if result:
                    print(f"âœ… ç¬¬ {cycle_count} æ¬¡å¾ªç¯å®Œæˆ")
                else:
                    print(f"âŒ ç¬¬ {cycle_count} æ¬¡å¾ªç¯å¤±è´¥")
                
                # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å¾ªç¯ï¼Œåˆ™ç­‰å¾…
                if self.running:
                    print(f"â³ ç­‰å¾… {interval_minutes} åˆ†é’Ÿåè¿›è¡Œä¸‹ä¸€æ¬¡å½•åˆ¶...")
                    print(f"ğŸ• ä¸‹æ¬¡å½•åˆ¶æ—¶é—´: {datetime.fromtimestamp(time.time() + interval_seconds).strftime('%Y-%m-%d %H:%M:%S')}")
                    
                    # åˆ†æ®µç­‰å¾…ï¼Œæ¯5ç§’æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦éœ€è¦é€€å‡º
                    for i in range(interval_seconds):
                        if not self.running:
                            break
                        time.sleep(1)
                        
                        # æ¯5ç§’æ˜¾ç¤ºä¸€æ¬¡å€’è®¡æ—¶
                        if (i + 1) % 5 == 0:
                            remaining = interval_seconds - (i + 1)
                            print(f"â° å‰©ä½™ç­‰å¾…æ—¶é—´: {remaining} ç§’")
                
            except KeyboardInterrupt:
                print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
                self.running = False
                break
            except Exception as e:
                print(f"âŒ å¾ªç¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                print("ğŸ”„ 5ç§’åé‡è¯•...")
                time.sleep(5)
        
        print("\nğŸ‘‹ ç¨‹åºå·²åœæ­¢")
        print(f"ğŸ“Š æ€»å…±å®Œæˆäº† {cycle_count} æ¬¡è§†é¢‘åˆ†æå¾ªç¯")

def try_open_camera(index, backend, result_dict):
    import cv2
    try:
        cap = cv2.VideoCapture(index, backend)
        if cap.isOpened():
            ret, frame = cap.read()
            if ret:
                width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                fps = cap.get(cv2.CAP_PROP_FPS)
                result_dict['success'] = True
                result_dict['width'] = width
                result_dict['height'] = height
                result_dict['fps'] = fps
        cap.release()
    except Exception:
        pass

def detect_cameras():
    """ç®€åŒ–ï¼šä»…è¿”å›é»˜è®¤å†…ç½®æ‘„åƒå¤´ï¼ˆç´¢å¼•0ï¼‰"""
    return [{
        'index': 0,
        'backend': cv2.CAP_ANY,
        'backend_name': 'Auto',
        'resolution': 'Auto',
        'fps': 30.0,
        'is_external': False,
        'display_name': 'é»˜è®¤å†…ç½®æ‘„åƒå¤´'
    }]

def select_camera():
    """ç®€åŒ–ï¼šç›´æ¥è¿”å›é»˜è®¤å†…ç½®æ‘„åƒå¤´ï¼ˆç´¢å¼•0ï¼‰"""
    return {
        'index': 0,
        'backend': cv2.CAP_ANY,
        'backend_name': 'Auto',
        'resolution': 'Auto',
        'fps': 30.0,
        'is_external': False,
        'display_name': 'é»˜è®¤å†…ç½®æ‘„åƒå¤´'
    }